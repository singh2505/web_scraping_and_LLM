# web_scraping_and_LLM

# Web Scraping and LLM Comparison with LoRA-based Quantization

This project demonstrates a workflow involving web scraping, comparing scraped content with text generated by a Large Language Model (LLM), and optimizing the LLM using LoRA-based quantization. The results are stored in a vector database for efficient similarity search.

## Table of Contents
- [Installation](#installation)
- [Usage](#usage)
- [Project Structure](#project-structure)
- [Steps and Code Explanation](#steps-and-code-explanation)
  - [Step 1: Install Necessary Libraries](#step-1-install-necessary-libraries)
  - [Step 2: Web Scraping](#step-2-web-scraping)
  - [Step 3: Compare with LLM Results](#step-3-compare-with-llm-results)
  - [Step 4: Store Results in a Vector Database](#step-4-store-results-in-a-vector-database)
  - [Step 5: Automate Query Handling](#step-5-automate-query-handling)
  - [Step 6: Apply LoRA-based Quantization](#step-6-apply-lora-based-quantization)


Step 1: Purpose: This code scrapes text content from a given URL (in this case, a Wikipedia page on web scraping), extracts the text from paragraph tags, and saves it to a file. It also displays the scraped content as an image.

step-2:Purpose: This code uses a pre-trained GPT-2 model to generate text based on a query ("Web scraping"). It saves the generated text to a file and displays it as an image.

step-3:Purpose: This code converts the scraped text and the LLM-generated text into vectors using a simple embedding method (summing up character codes). It then stores these vectors in a FAISS index, which is used for efficient similarity search.

step:4-Purpose: This code searches the FAISS index for the vector most similar to a query vector and returns the index of the closest match. This is useful for quickly finding similar content based on vector embeddings.

step-5-Purpose: This code configures and applies LoRA (Low-Rank Adaptation) to the model to make it more parameter-efficient. It benchmarks the model's performance before and after applying quantization, demonstrating the impact on inference time. Quantization reduces the model size and speeds up inference by converting model weights to a lower precision.

## Installation

To get started, clone this repository and install the necessary libraries:

```sh
git clone <repository-url>
cd <repository-directory>
pip install beautifulsoup4 requests transformers faiss-cpu peft torch






